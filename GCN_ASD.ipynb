{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE7XFBYOhB8U"
      },
      "outputs": [],
      "source": [
        "#@title GCN Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATv2Conv, GINConv,  global_mean_pool,global_max_pool, global_add_pool, TopKPooling\n",
        "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
        "\n",
        "def graph_readout(x,batch,  method):\n",
        "\n",
        "    if method == 'mean':\n",
        "        return global_mean_pool(x,batch)\n",
        "\n",
        "    elif method == 'meanmax':\n",
        "        x_mean = global_mean_pool(x,batch)\n",
        "        x_max = global_max_pool(x,batch)\n",
        "        return torch.cat((x_mean, x_max),1)\n",
        "\n",
        "    elif method == 'sum':\n",
        "        return global_add_pool(x,batch)\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Undefined readout opertaion')\n",
        "\n",
        "\n",
        "class Abstract_GNN(torch.nn.Module):\n",
        "    def __init__(self, num_nodes, readout):\n",
        "        super(Abstract_GNN, self).__init__()\n",
        "        self.readout = readout\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "            for p in self.parameters():\n",
        "                #print(p)\n",
        "                if p.dim() > 1:\n",
        "                    nn.init.xavier_uniform_(p)\n",
        "                else:\n",
        "                    nn.init.uniform_(p)\n",
        "\n",
        "    def forward(self,data):\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class GCN(Abstract_GNN):\n",
        "    def __init__(self, num_nodes, readout, **kwargs):\n",
        "        super().__init__(num_nodes, readout)\n",
        "        self.f1 = 64\n",
        "        self.f2 = 32\n",
        "        self.readout = readout\n",
        "\n",
        "\n",
        "        self.conv1 = GCNConv(num_nodes, self.f1)\n",
        "        self.pool1 = TopKPooling(self.f1, ratio=0.5)\n",
        "        self.conv2 = GCNConv(self.f1, self.f2)\n",
        "        self.pool2 = TopKPooling(self.f2, ratio=0.5)\n",
        "        last_dim = 2 if readout=='meanmax' else 1\n",
        "        self.fc1 = nn.Linear((self.f2 + self.f1) * last_dim,  32)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(32)\n",
        "        self.fc2 = torch.nn.Linear(32, 2)\n",
        "\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index,edge_weight, batch, pos = data.x, data.edge_index, data.edge_attr, data.batch, data.pos\n",
        "        x = self.conv1(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x, edge_index, edge_weight, batch, perm1, score1 = self.pool1(x, edge_index, edge_weight, batch)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x1 = graph_readout(x, batch, self.readout)\n",
        "        pos = pos[perm1]\n",
        "        s1 = zip(perm1.tolist(), score1.tolist(), batch.tolist(), pos.tolist())\n",
        "        s1_cnt = {}\n",
        "        for p, s, b, po in s1:\n",
        "          k = 0\n",
        "          for i in range(len(po)):\n",
        "            if po[i] == 1:\n",
        "              k = i\n",
        "          if k in s1_cnt.keys():\n",
        "            s1_cnt[k] += s\n",
        "          else:\n",
        "            s1_cnt[k] = s\n",
        "\n",
        "        s1 = sorted(s1_cnt.items(), key = lambda x: x[1])\n",
        "\n",
        "\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        x= F.relu(x)\n",
        "        x, edge_index, edge_weight, batch, perm2, score2 = self.pool2(x, edge_index, edge_weight, batch)\n",
        "        pos = pos[perm2]\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        s2 = zip(perm1[perm2].tolist(), score2.tolist(), batch.tolist(), pos.tolist())\n",
        "        s2_cnt = {}\n",
        "\n",
        "        for p, s, b, po in s2:\n",
        "          k = 0\n",
        "          for i in range(len(po)):\n",
        "            if po[i] == 1:\n",
        "              k = i\n",
        "          if k in s2_cnt.keys():\n",
        "            s2_cnt[k] += s\n",
        "          else:\n",
        "            s2_cnt[k] = s\n",
        "        s2 = sorted(s2_cnt.items(), key = lambda x: x[1])\n",
        "\n",
        "\n",
        "        x2 = graph_readout(x, batch, self.readout)\n",
        "        x = torch.cat([x1, x2], dim = 1)\n",
        "        x = self.bn1(F.relu(self.fc1(x)))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.log_softmax(x, dim=-1)\n",
        "\n",
        "        s1, _ = zip(*s1)\n",
        "        s2, _ = zip(*s2)\n",
        "        return x, s1[-20:], s2[-20:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCyq9KaQVq0l"
      },
      "outputs": [],
      "source": [
        "#@title Data Preparation Functions\n",
        "import os.path as osp\n",
        "from os import listdir\n",
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from torch_geometric.data import Data\n",
        "import networkx as nx\n",
        "import networkx.convert_matrix\n",
        "import multiprocessing\n",
        "from torch_sparse import coalesce\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "from functools import partial\n",
        "import deepdish as dd\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def split(data, batch):\n",
        "    node_slice = torch.cumsum(torch.from_numpy(np.bincount(batch)), 0)\n",
        "    node_slice = torch.cat([torch.tensor([0]), node_slice])\n",
        "\n",
        "    row, _ = data.edge_index\n",
        "    edge_slice = torch.cumsum(torch.from_numpy(np.bincount(batch[row])), 0)\n",
        "    edge_slice = torch.cat([torch.tensor([0]), edge_slice])\n",
        "\n",
        "    data.edge_index -= node_slice[batch[row]].unsqueeze(0)\n",
        "\n",
        "    slices = {'edge_index': edge_slice}\n",
        "    if data.x is not None:\n",
        "        slices['x'] = node_slice\n",
        "    if data.edge_attr is not None:\n",
        "        slices['edge_attr'] = edge_slice\n",
        "    if data.y is not None:\n",
        "        if data.y.size(0) == batch.size(0):\n",
        "            slices['y'] = node_slice\n",
        "        else:\n",
        "            slices['y'] = torch.arange(0, batch[-1] + 2, dtype=torch.long)\n",
        "    if data.pos is not None:\n",
        "        slices['pos'] = node_slice\n",
        "\n",
        "    return data, slices\n",
        "\n",
        "\n",
        "def cat(seq):\n",
        "    seq = [item for item in seq if item is not None]\n",
        "    seq = [item.unsqueeze(-1) if item.dim() == 1 else item for item in seq]\n",
        "    return torch.cat(seq, dim=-1).squeeze() if len(seq) > 0 else None\n",
        "\n",
        "class NoDaemonProcess(multiprocessing.Process):\n",
        "    @property\n",
        "    def daemon(self):\n",
        "        return False\n",
        "\n",
        "    @daemon.setter\n",
        "    def daemon(self, value):\n",
        "        pass\n",
        "\n",
        "\n",
        "class NoDaemonContext(type(multiprocessing.get_context())):\n",
        "    Process = NoDaemonProcess\n",
        "\n",
        "\n",
        "def read_data(data_dir):\n",
        "    onlyfiles = [f for f in listdir(data_dir) if osp.isfile(osp.join(data_dir, f))]\n",
        "    onlyfiles.sort()\n",
        "    batch = []\n",
        "    pseudo = []\n",
        "    y_list = []\n",
        "    edge_att_list, edge_index_list,att_list = [], [], []\n",
        "\n",
        "    func = partial(read_sigle_data, data_dir)\n",
        "\n",
        "    import timeit\n",
        "\n",
        "    res = []\n",
        "    for i in range(len(onlyfiles)):\n",
        "      res.append(func(onlyfiles[i]))\n",
        "\n",
        "\n",
        "    for j in range(len(res)):\n",
        "\n",
        "        edge_att_list.append(res[j][0])\n",
        "        edge_index_list.append(res[j][1]+j*res[j][4])\n",
        "        att_list.append(res[j][2])\n",
        "        y_list.append(res[j][3])\n",
        "        batch.append([j]*res[j][4])\n",
        "        pseudo.append(np.diag(np.ones(res[j][4])))\n",
        "\n",
        "    edge_att_arr = np.concatenate(edge_att_list)\n",
        "    edge_index_arr = np.concatenate(edge_index_list, axis=1)\n",
        "    att_arr = np.concatenate(att_list, axis=0)\n",
        "    pseudo_arr = np.concatenate(pseudo, axis=0)\n",
        "    y_arr = np.stack(y_list)\n",
        "    edge_att_torch = torch.from_numpy(edge_att_arr.reshape(len(edge_att_arr), 1)).float()\n",
        "    att_torch = torch.from_numpy(att_arr).float()\n",
        "    y_torch = torch.from_numpy(y_arr).long()  # classification\n",
        "    batch_torch = torch.from_numpy(np.hstack(batch)).long()\n",
        "    edge_index_torch = torch.from_numpy(edge_index_arr).long()\n",
        "    pseudo_torch = torch.from_numpy(pseudo_arr).float()\n",
        "    data = Data(x=att_torch, edge_index=edge_index_torch, y=y_torch, edge_attr=edge_att_torch, pos = pseudo_torch)\n",
        "\n",
        "\n",
        "    data, slices = split(data, batch_torch)\n",
        "\n",
        "    return data, slices\n",
        "\n",
        "\n",
        "\n",
        "def read_sigle_data(data_dir,filename):\n",
        "\n",
        "    temp = dd.io.load(osp.join(data_dir, filename))\n",
        "    subject_id = filename[:5]\n",
        "    node_att = pd.DataFrame([])\n",
        "\n",
        "    file_path = os.path.join(data_dir[:-3], subject_id)\n",
        "    ro_file = [f for f in os.listdir(file_path) if f.endswith('.1D')]\n",
        "    file_path = os.path.join(file_path, ro_file[0])\n",
        "    ho_rois = pd.read_csv(file_path, sep='\\t').iloc[:78, :].T\n",
        "    node_att = pd.concat([node_att, ho_rois])\n",
        "    node_att = torch.tensor(node_att.values)\n",
        "    pcorr = np.abs(temp['corr'][()])\n",
        "\n",
        "    num_nodes = pcorr.shape[0]\n",
        "    G = nx.DiGraph(pcorr)\n",
        "    A = nx.to_scipy_sparse_array(G)\n",
        "    adj = A.tocoo()\n",
        "    edge_att = np.zeros(len(adj.row))\n",
        "    for i in range(len(adj.row)):\n",
        "        edge_att[i] = pcorr[adj.row[i], adj.col[i]]\n",
        "\n",
        "    edge_index = np.stack([adj.row, adj.col])\n",
        "    edge_index, edge_att = remove_self_loops(torch.from_numpy(edge_index), torch.from_numpy(edge_att))\n",
        "    edge_index = edge_index.long()\n",
        "    edge_index, edge_att = coalesce(edge_index, edge_att, num_nodes,\n",
        "                                    num_nodes)\n",
        "    label = temp['label'][()]\n",
        "\n",
        "    y_torch = torch.from_numpy(np.array(label)).long()  # classification\n",
        "\n",
        "    data = Data(x=node_att, edge_index=edge_index.long(), y=y_torch, edge_attr=edge_att)\n",
        "    # return data\n",
        "    return [edge_att.data.numpy(),edge_index.data.numpy(), node_att,label,num_nodes]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train-Validation Split\n",
        "\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.io import loadmat\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "def train_val_test_split(n_sub, kfold = 10, fold = 0):\n",
        "    id = list(range(n_sub))\n",
        "\n",
        "\n",
        "    import random\n",
        "    random.seed(42)\n",
        "    random.shuffle(id)\n",
        "\n",
        "    kf = KFold(n_splits=kfold, random_state=123,shuffle = True)\n",
        "    kf2 = KFold(n_splits=kfold-1, shuffle=True, random_state = 666)\n",
        "\n",
        "\n",
        "    test_index = list()\n",
        "    train_index = list()\n",
        "    val_index = list()\n",
        "\n",
        "    for tr,te in kf.split(np.array(id)):\n",
        "        test_index.append(te)\n",
        "        tr_id, val_id = list(kf2.split(tr))[0]\n",
        "        train_index.append(tr[tr_id])\n",
        "        val_index.append(tr[val_id])\n",
        "\n",
        "    train_id = train_index[fold]\n",
        "    test_id = test_index[fold]\n",
        "    val_id = val_index[fold]\n",
        "\n",
        "    return train_id,val_id,test_id"
      ],
      "metadata": {
        "id": "E6eDYZzSNENB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjCiHiZ1VvQ9"
      },
      "outputs": [],
      "source": [
        "#@title ABIDE Data Loader\n",
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset,Data\n",
        "from os.path import join, isfile\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "\n",
        "\n",
        "\n",
        "class ABIDEDataset(InMemoryDataset):\n",
        "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
        "\n",
        "        self.root = root\n",
        "        self.name = name\n",
        "        super(ABIDEDataset, self).__init__(root,transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        data_dir = osp.join(self.root,'raw')\n",
        "        onlyfiles = [f for f in listdir(data_dir) if osp.isfile(osp.join(data_dir, f))]\n",
        "        onlyfiles.sort()\n",
        "        return onlyfiles\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return  'data.pt'\n",
        "\n",
        "    def download(self):\n",
        "        return\n",
        "\n",
        "    def process(self):\n",
        "        self.data, self.slices = read_data(self.raw_dir)\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [self.get(idx) for idx in range(len(self))]\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "            self.data, self.slices = self.collate(data_list)\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.get(idx) for idx in range(len(self))]\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "            self.data, self.slices = self.collate(data_list)\n",
        "\n",
        "        torch.save((self.data, self.slices), self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({})'.format(self.name, len(self))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7nmNBLmTshk"
      },
      "outputs": [],
      "source": [
        "# @title Model Training\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "EPS = 1e-10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--epoch', type=int, default=0, help='starting epoch')\n",
        "parser.add_argument('--n_epochs', type=int, default=100, help='number of epochs of training')\n",
        "parser.add_argument('--batchSize', type=int, default=50, help='size of the batches')\n",
        "parser.add_argument('--dataroot', type=str, default='..', help='root directory of the dataset')\n",
        "parser.add_argument('--fold', type=int, default=0, help='training which fold')\n",
        "parser.add_argument('--lr', type = float, default=0.001, help='learning rate')\n",
        "parser.add_argument('--stepsize', type=int, default=20, help='scheduler step size')\n",
        "parser.add_argument('--gamma', type=float, default=0.7, help='scheduler shrinking rate')\n",
        "parser.add_argument('--weightdecay', type=float, default=1e-2, help='regularization')\n",
        "parser.add_argument('--lamb0', type=float, default=1, help='classification loss weight')\n",
        "parser.add_argument('--layer', type=int, default=2, help='number of GNN layers')\n",
        "parser.add_argument('--ratio', type=float, default=0.5, help='pooling ratio')\n",
        "parser.add_argument('--indim', type=int, default=78, help='feature dim')\n",
        "parser.add_argument('--nroi', type=int, default=200, help='num of ROIs')\n",
        "parser.add_argument('--nclass', type=int, default=2, help='num of classes')\n",
        "parser.add_argument('--load_model', type=bool, default=False)\n",
        "parser.add_argument('--save_model', type=bool, default=True)\n",
        "parser.add_argument('--optim', type=str, default='Adam', help='optimization method: SGD, Adam')\n",
        "parser.add_argument('--save_path', type=str, default='./model/', help='path to save model')\n",
        "opt = parser.parse_args(args=[])\n",
        "\n",
        "if not os.path.exists(opt.save_path):\n",
        "    os.makedirs(opt.save_path)\n",
        "\n",
        "#################### Parameter Initialization #######################\n",
        "path = opt.dataroot\n",
        "name = 'ABIDE'\n",
        "\n",
        "load_model = opt.load_model\n",
        "opt_method = opt.optim\n",
        "num_epoch = opt.n_epochs\n",
        "fold = opt.fold\n",
        "\n",
        "\n",
        "\n",
        "################## Define Dataloader ##################################\n",
        "dataset = ABIDEDataset(path,name)\n",
        "\n",
        "dataset.data.y = dataset.data.y.squeeze()\n",
        "dataset.data.x[dataset.data.x == float('inf')] = 0\n",
        "tr_index,val_index,te_index = train_val_test_split(n_sub = dataset.data.y.shape[0], fold=fold)\n",
        "train_dataset = dataset[tr_index]\n",
        "\n",
        "val_dataset = dataset[list(val_index) + list(te_index)]\n",
        "test_dataset = dataset[list(val_index) + list(te_index)]\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=opt.batchSize, shuffle= True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=opt.batchSize, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=opt.batchSize, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "############### Define Graph Deep Learning Network ##########################\n",
        "model = GCN(78, \"meanmax\").to(device)\n",
        "print(model)\n",
        "\n",
        "if opt_method == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr= opt.lr, weight_decay=opt.weightdecay)\n",
        "elif opt_method == 'SGD':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr =opt.lr, momentum = 0.9, weight_decay=opt.weightdecay, nesterov = True)\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.stepsize, gamma=opt.gamma)\n",
        "\n",
        "save_model = True\n",
        "\n",
        "###################### Network Training Function#####################################\n",
        "def train(epoch):\n",
        "    print('train...........')\n",
        "    scheduler.step()\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print(\"LR\", param_group['lr'])\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    #train by batch\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, s1, s2 = model(data)\n",
        "        loss_c = F.nll_loss(output, data.y)\n",
        "        loss = opt.lamb0*loss_c\n",
        "\n",
        "        loss.backward()\n",
        "        loss_all += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "\n",
        "    return loss_all / len(train_dataset)\n",
        "\n",
        "\n",
        "###################### Network Testing Function#####################################\n",
        "def test_acc(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    topkdict = {}\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        outputs, s1, s2= model(data)\n",
        "        for i in s2:\n",
        "          i = int(i)\n",
        "          if i in topkdict.keys():\n",
        "            topkdict[i]+=1\n",
        "          else:\n",
        "            topkdict[i] = 1\n",
        "        pred = outputs.max(dim=1)[1]\n",
        "        correct += pred.eq(data.y).sum().item()\n",
        "    #selected biomarkers\n",
        "    print(\"selected:\", sorted(topkdict.items(), key=lambda\n",
        "                 kv:kv[1])[-5:])\n",
        "\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "def test_loss(loader,epoch):\n",
        "    print('testing...........')\n",
        "    model.eval()\n",
        "    loss_all = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        output, s1, s2= model(data)\n",
        "        loss_c = F.nll_loss(output, data.y)\n",
        "        loss = opt.lamb0*loss_c\n",
        "\n",
        "        loss_all += loss.item() * data.num_graphs\n",
        "    return loss_all / len(loader.dataset)\n",
        "\n",
        "#######################################################################################\n",
        "############################   Model Training #########################################\n",
        "#######################################################################################\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = 1e10\n",
        "best_acc = 0\n",
        "indicator = 0\n",
        "\n",
        "for epoch in range(0, num_epoch):\n",
        "    if indicator > 20:\n",
        "        break\n",
        "    indicator += 1\n",
        "    since  = time.time()\n",
        "    tr_loss= train(epoch)\n",
        "    tr_acc = test_acc(train_loader)\n",
        "    val_acc = test_acc(val_loader)\n",
        "    val_loss = test_loss(val_loader,epoch)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('*====**')\n",
        "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Epoch: {:03d}, Train Loss: {:.7f}, '\n",
        "          'Train Acc: {:.7f}, Test Loss: {:.7f}, Test Acc: {:.7f}'.format(epoch, tr_loss,\n",
        "                                                       tr_acc, val_loss, val_acc))\n",
        "\n",
        "    if val_acc > best_acc and epoch > 5:\n",
        "        indicator = 0\n",
        "        print(\"saving best model\")\n",
        "        best_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if save_model:\n",
        "            torch.save(best_model_wts, os.path.join(\"../.pt\"))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Confusion Matrix\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "EPS = 1e-10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--epoch', type=int, default=0, help='starting epoch')\n",
        "parser.add_argument('--n_epochs', type=int, default=100, help='number of epochs of training')\n",
        "parser.add_argument('--batchSize', type=int, default=50, help='size of the batches')\n",
        "parser.add_argument('--dataroot', type=str, default='..', help='root directory of the dataset')\n",
        "parser.add_argument('--fold', type=int, default=0, help='training which fold')\n",
        "parser.add_argument('--lr', type = float, default=0.001, help='learning rate')\n",
        "parser.add_argument('--stepsize', type=int, default=20, help='scheduler step size')\n",
        "parser.add_argument('--gamma', type=float, default=0.7, help='scheduler shrinking rate')\n",
        "parser.add_argument('--weightdecay', type=float, default=1e-2, help='regularization')\n",
        "parser.add_argument('--lamb0', type=float, default=1, help='classification loss weight')\n",
        "parser.add_argument('--layer', type=int, default=2, help='number of GNN layers')\n",
        "parser.add_argument('--ratio', type=float, default=0.5, help='pooling ratio')\n",
        "parser.add_argument('--indim', type=int, default=78, help='feature dim')\n",
        "parser.add_argument('--nroi', type=int, default=200, help='num of ROIs')\n",
        "parser.add_argument('--nclass', type=int, default=2, help='num of classes')\n",
        "parser.add_argument('--load_model', type=bool, default=False)\n",
        "parser.add_argument('--save_model', type=bool, default=True)\n",
        "parser.add_argument('--optim', type=str, default='Adam', help='optimization method: SGD, Adam')\n",
        "parser.add_argument('--save_path', type=str, default='./model/', help='path to save model')\n",
        "opt = parser.parse_args(args=[])\n",
        "\n",
        "\n",
        "#################### Parameter Initialization #######################\n",
        "path = opt.dataroot\n",
        "name = 'ABIDE'\n",
        "\n",
        "load_model = opt.load_model\n",
        "opt_method = opt.optim\n",
        "num_epoch = opt.n_epochs\n",
        "fold = opt.fold\n",
        "\n",
        "\n",
        "################## Define Dataloader ##################################\n",
        "dataset = ABIDEDataset(path,name)\n",
        "\n",
        "dataset.data.y = dataset.data.y.squeeze()\n",
        "dataset.data.x[dataset.data.x == float('inf')] = 0\n",
        "tr_index,val_index,te_index = train_val_test_split(n_sub = dataset.data.y.shape[0], fold=fold)\n",
        "train_dataset = dataset[tr_index]\n",
        "\n",
        "val_dataset = dataset[list(val_index) + list(te_index)]\n",
        "test_dataset = dataset[list(val_index) + list(te_index)]\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=opt.batchSize, shuffle= True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=opt.batchSize, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=opt.batchSize, shuffle=False)\n",
        "\n",
        "############ Confusion Matrix and Evaluatation Metrics ################\n",
        "model = GCN(78, \"meanmax\").to(device)\n",
        "model.load_state_dict(torch.load(\".pt\"))\n",
        "model.eval()\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "correct = 0\n",
        "for data in val_loader:\n",
        "    data = data.to(device)\n",
        "    outputs,s1,s2= model(data)\n",
        "    pred = outputs.max(1)[1]\n",
        "    preds.append(pred.cpu().detach().numpy())\n",
        "    trues.append(data.y.cpu().detach().numpy())\n",
        "    correct += pred.eq(data.y).sum().item()\n",
        "preds = np.concatenate(preds,axis=0)\n",
        "trues = np.concatenate(trues,axis = 0)\n",
        "cm = confusion_matrix(trues,preds, labels = [0, 1])\n",
        "print(\"Confusion matrix\")\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[0, 1])\n",
        "disp.plot()\n",
        "plt.show()\n",
        "print(classification_report(trues, preds))\n",
        "\n"
      ],
      "metadata": {
        "id": "xjj82_Dxv5lB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}